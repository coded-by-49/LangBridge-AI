{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-kSDQysHIn6"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8Zj65i7HIn9"
      },
      "outputs": [],
      "source": [
        "henryalohfabian_langbridge_wazobia_dataset_path = kagglehub.dataset_download('henryalohfabian/langbridge-wazobia-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PgRMRssHIn-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install trl\n",
        "!pip install accelerate\n",
        "!pip install peft\n",
        "!pip install bitsandbytes\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install safetensors\n",
        "!pip install -q trl peft accelerate bitsandbytes\n",
        "!pip install -q \"transformers>=4.39.3\" \"datasets>=2.18.0\"\n",
        "\n",
        "# 1. Core Hugging Face Libraries\n",
        "!pip install -q -U transformers accelerate datasets peft bitsandbytes\n",
        "\n",
        "# 2. Evaluation & Metrics (CRITICAL for Translation)\n",
        "!pip install -q evaluate sacrebleu\n",
        "\n",
        "# 3. Tokenizer Support (CRITICAL for NLLB)\n",
        "!pip install -q sentencepiece protobuf\n",
        "\n",
        "# 4. Monitoring (Optional but recommended for Kaggle)\n",
        "!pip install -q wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-01T19:43:46.395916Z",
          "iopub.status.busy": "2025-12-01T19:43:46.395097Z",
          "iopub.status.idle": "2025-12-01T19:43:51.280079Z",
          "shell.execute_reply": "2025-12-01T19:43:51.278724Z",
          "shell.execute_reply.started": "2025-12-01T19:43:46.395884Z"
        },
        "id": "njX4pG75HIn_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-02T00:01:08.784975Z",
          "iopub.status.busy": "2025-12-02T00:01:08.784774Z",
          "iopub.status.idle": "2025-12-02T00:01:08.791144Z",
          "shell.execute_reply": "2025-12-02T00:01:08.790674Z",
          "shell.execute_reply.started": "2025-12-02T00:01:08.784949Z"
        },
        "id": "9wo7_Bh5HIoA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-01T19:43:52.783424Z",
          "iopub.status.busy": "2025-12-01T19:43:52.782921Z",
          "iopub.status.idle": "2025-12-01T19:43:52.855056Z",
          "shell.execute_reply": "2025-12-01T19:43:52.853262Z",
          "shell.execute_reply.started": "2025-12-01T19:43:52.783376Z"
        },
        "id": "ry_cIuMLHIoB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "print(\"Seq2SeqTrainer imported successfully!\")\n",
        "print(f\"Torch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "-2kmO0pBHIoB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Standard Library & Hardware Setup\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "import os\n",
        "import gc  # Garbage collection for memory management\n",
        "import torch\n",
        "import numpy as np\n",
        "from colorama import Fore, Style\n",
        "\n",
        "# Set randomness for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Quick Hardware Check\n",
        "print(f\"{Fore.CYAN}--- Environment Status ---{Style.RESET_ALL}\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Detected: {Fore.GREEN}{torch.cuda.get_device_name(0)}{Style.RESET_ALL}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(f\"{Fore.RED}WARNING: No GPU detected. Training will be extremely slow.{Style.RESET_ALL}\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Hugging Face Datasets (Data Loading)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# load_from_disk is critical for your pre-saved Arrow shards\n",
        "from datasets import load_from_disk, load_dataset, DatasetDict\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Transformers (Model & Tokenizer)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,      # <--- STRICTLY for NLLB (Encoder-Decoder)\n",
        "    BitsAndBytesConfig,         # For 4-bit Quantization (QLoRA)\n",
        "    DataCollatorForSeq2Seq,     # Dynamic padding for translation batches\n",
        "    Seq2SeqTrainer,             # The specialized trainer for Translation\n",
        "    Seq2SeqTrainingArguments,   # Arguments specific to generation tasks\n",
        "    TrainerCallback,            # For your custom printer\n",
        "    GenerationConfig            # To control inference parameters during eval\n",
        ")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 4. PEFT (Parameter-Efficient Fine-Tuning)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    TaskType,                   # To specify SEQ_2_SEQ_LM\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel                   # Useful for loading adapters later\n",
        ")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 5. Evaluation Metrics\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "import evaluate\n",
        "\n",
        "# SacreBLEU is the standard for Machine Translation evaluation\n",
        "# It requires: pip install sacrebleu\n",
        "try:\n",
        "    metric = evaluate.load(\"sacrebleu\")\n",
        "    print(f\"{Fore.GREEN}Metric 'sacrebleu' loaded successfully.{Style.RESET_ALL}\")\n",
        "except ImportError:\n",
        "    print(f\"{Fore.RED}Error: 'sacrebleu' library missing. Run: pip install sacrebleu{Style.RESET_ALL}\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 6. Weights & Biases (Optional but Recommended for Logging)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Ensures you can visualize loss curves even if Kaggle disconnects\n",
        "import wandb\n",
        "# wandb.login(key=\"YOUR_KEY_HERE\") # Uncomment if you have a key\n",
        "\n",
        "print(f\"{Fore.CYAN}--- Import Complete ---{Style.RESET_ALL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAvcfBT4HIoC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True, # selection of quantization level\n",
        "    bnb_4bit_quant_type=\"nf4\", # this parameter retains the quality of data even upon reduction of the it bits\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-01T20:10:28.004493Z",
          "iopub.status.busy": "2025-12-01T20:10:28.003744Z",
          "iopub.status.idle": "2025-12-01T20:10:28.008543Z",
          "shell.execute_reply": "2025-12-01T20:10:28.007746Z",
          "shell.execute_reply.started": "2025-12-01T20:10:28.004465Z"
        },
        "id": "uj2o9UorHIoD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    inference_mode=False,\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"v_proj\",\n",
        "        \"k_proj\",\n",
        "        \"out_proj\"\n",
        "    ],\n",
        "    bias=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8k5JnQ8HIoE",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-01T20:09:05.875293Z",
          "iopub.status.busy": "2025-12-01T20:09:05.874723Z",
          "iopub.status.idle": "2025-12-01T20:09:31.298286Z",
          "shell.execute_reply": "2025-12-01T20:09:31.29742Z",
          "shell.execute_reply.started": "2025-12-01T20:09:05.875266Z"
        },
        "id": "GZ8AUaEpHIoE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "device_map = {\"\": torch.cuda.current_device()}\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map= device_map,\n",
        "    quantization_config = quant_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pChlxf8pHIoF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-01T20:10:13.012735Z",
          "iopub.status.busy": "2025-12-01T20:10:13.012272Z",
          "iopub.status.idle": "2025-12-01T20:10:18.932291Z",
          "shell.execute_reply": "2025-12-01T20:10:18.931446Z",
          "shell.execute_reply.started": "2025-12-01T20:10:13.012692Z"
        },
        "id": "CJkFTA-zHIoF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    label_pad_token_id=-100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-01T20:10:03.11632Z",
          "iopub.status.busy": "2025-12-01T20:10:03.116009Z",
          "iopub.status.idle": "2025-12-01T20:10:07.608675Z",
          "shell.execute_reply": "2025-12-01T20:10:07.607853Z",
          "shell.execute_reply.started": "2025-12-01T20:10:03.116296Z"
        },
        "id": "3h1ncCLFHIoF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "import os\n",
        "\n",
        "\n",
        "dataset_path = \"/kaggle/input/langbridge-wazobia-dataset/nllb-training-data-merged/content/drive/MyDrive/data/NLLB_FINAL_TRAIN_DATA\"\n",
        "\n",
        "if os.path.exists(dataset_path):\n",
        "    print(\"Files in this folder:\", os.listdir(dataset_path))\n",
        "    train_dataset = load_from_disk(dataset_path)\n",
        "    print(\"Success! Dataset loaded.\")\n",
        "else:\n",
        "    print(f\"Directory not found: {dataset_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWKTNaXVHIoG"
      },
      "source": [
        "# 5. METRICS & CALLBACKS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K35H0CsUHIoH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple): preds = preds[0]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in labels\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {\"bleu\": result[\"score\"]}\n",
        "\n",
        "class PrinterCallback(TrainerCallback):\n",
        "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
        "        print(f\"\\n--- SAMPLE TRANSLATION (Step {state.global_step}) ---\")\n",
        "        device = model.device\n",
        "        # Grab first 2 examples from val set\n",
        "        inputs = [val_dataset[i] for i in range(2)]\n",
        "\n",
        "        # Manually collate\n",
        "        from torch.nn.utils.rnn import pad_sequence\n",
        "        input_ids = [torch.tensor(x['input_ids']) for x in inputs]\n",
        "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Force max length to prevent infinite loops\n",
        "            gen_ids = model.generate(input_ids=input_ids, max_new_tokens=60)\n",
        "\n",
        "        in_text = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
        "        out_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
        "\n",
        "        for i in range(len(in_text)):\n",
        "            print(f\"In : {in_text[i]}\")\n",
        "            print(f\"Out: {out_text[i]}\")\n",
        "            print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-01T20:32:43.710879Z",
          "iopub.status.busy": "2025-12-01T20:32:43.710204Z",
          "iopub.status.idle": "2025-12-01T20:32:43.997164Z",
          "shell.execute_reply": "2025-12-01T20:32:43.996533Z",
          "shell.execute_reply.started": "2025-12-01T20:32:43.710854Z"
        },
        "id": "7Q_GRLQdHIoI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "val_path = \"/kaggle/input/langbridge-wazobia-dataset/igbo_test_val-20251130T173420Z-1-001/igbo_test_val\"\n",
        "\n",
        "if os.path.exists(val_path):\n",
        "    val_dataset = load_from_disk(val_path)\n",
        "    print(f\"Validation set loaded from: {val_path}\")\n",
        "else:\n",
        "  \n",
        "    print(\"Specific val set not found\")\n",
        "\n",
        "print(f\"Training on {len(train_dataset)} examples.\")\n",
        "print(f\"Validating on {len(val_dataset)} examples (Igbo Only).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPosstBSHIoI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = \"/kaggle/working/langbridge_AI\"\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"coded-by-49/Lang_bridge_AI\",\n",
        "    hub_strategy=\"checkpoint\",\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"steps\", \n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",       \n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    fp16=True,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",            \n",
        "    predict_with_generate=True,  \n",
        "    generation_max_length=60,\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,    \n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics, \n",
        "    callbacks=[PrinterCallback()]     \n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# --- 7. EXECUTION (CRASH PROOF WRAPPER) ---\n",
        "import traceback\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\\nCRITICAL ERROR DURING TRAINING: {e}\")\n",
        "    print(traceback.format_exc())\n",
        "    print(\"\\nAttempting to save latest checkpoint before exiting...\")\n",
        "\n",
        "    # Attempt to save whatever we have\n",
        "    trainer.save_model(os.path.join(OUTPUT_DIR, \"crash_salvage_adapter\"))\n",
        "    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"crash_salvage_adapter\"))\n",
        "    print(\"Salvage successful. Exiting gracefully so Kaggle saves the output.\")\n",
        "\n",
        "# --- 8. NORMAL SAVING ---\n",
        "# This runs regardless of success or failure\n",
        "print(\"Saving final artifacts...\")\n",
        "final_save_path = os.path.join(OUTPUT_DIR, \"final_adapter\")\n",
        "trainer.model.save_pretrained(final_save_path)\n",
        "tokenizer.save_pretrained(final_save_path)\n",
        "print(f\"DONE! Model saved to: {final_save_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "lang_bridge_training_process",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 8881328,
          "sourceId": 13936048,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31192,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
